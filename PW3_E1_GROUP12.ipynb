{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 1: Preparación del Medio Ambiente y Carga de Datos\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Cargar el dataset desde el archivo JSON proporcionado\n",
    "with open('pokemons-1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Aplanar la estructura del JSON para convertirlo en un DataFrame de Pandas\n",
    "# Extraemos el nombre, las estadísticas (stats) y el tipo elemental principal\n",
    "rows = []\n",
    "for name, info in data['pokedex'].items():\n",
    "    row = {\n",
    "        'name': name,\n",
    "        'hp': info['stats']['hp'],\n",
    "        'atk': info['stats']['atk'],\n",
    "        'def': info['stats']['def'],\n",
    "        'spa': info['stats']['spa'],\n",
    "        'spd': info['stats']['spd'],\n",
    "        'spe': info['stats']['spe'],\n",
    "        'type1': info['types'][0],  # Tomamos el primer tipo como referencia\n",
    "        'hasEvo': info['hasEvo']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Dataset cargado con {df.shape[0]} Pokémon y {df.shape[1]} características.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 1.1: Análisis Exploratorio de Datos (EDA)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas de los rasgos fisiológicos:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Visualización 1: Distribución de las estadísticas principales\n",
    "plt.figure(figsize=(12, 6))\n",
    "df[['hp', 'atk', 'def', 'spa', 'spd', 'spe']].boxplot()\n",
    "plt.title('Distribución de Rasgos Fisiológicos de los Pokémon')\n",
    "plt.ylabel('Valor de la estadística')\n",
    "plt.show()\n",
    "\n",
    "# Visualización 2: Correlación entre atributos\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(df[['hp', 'atk', 'def', 'spa', 'spd', 'spe']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Matriz de Correlación entre Estadísticas')\n",
    "plt.show()\n",
    "\n",
    "# Visualización 3: Conteo por tipo elemental\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.countplot(data=df, x='type1', order=df['type1'].value_counts().index)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribución de Pokémon por Tipo Elemental Principal')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 1.2: Preprocesamiento y Transformación\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Seleccionamos solo los rasgos fisiológicos para el agrupamiento\n",
    "# No incluimos el nombre ni el tipo, ya que el objetivo es ver si se agrupan solos\n",
    "features = ['hp', 'atk', 'def', 'spa', 'spd', 'spe']\n",
    "X = df[features]\n",
    "\n",
    "# Escalado de datos: Es crítico para clustering basado en distancias (K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Comprobar si hay valores nulos en el DataFrame\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# También puedes ver el total general\n",
    "total_nans = df.isna().sum().sum()\n",
    "print(f\"\\nTotal de valores nulos en todo el dataset: {total_nans}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 2: Implementación - K-Means\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Método del Codo (Elbow Method) para encontrar el K óptimo\n",
    "inertia = []\n",
    "K_range = range(2, 15)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertia, 'bx-')\n",
    "plt.xlabel('Número de clusters (k)')\n",
    "plt.ylabel('Inercia (Suma de errores cuadrados)')\n",
    "plt.title('Método del Codo para Selección de K')\n",
    "plt.show()\n",
    "\n",
    "# Aplicamos K-Means con el K óptimo (supongamos K=6 basándonos en tipos o el codo)\n",
    "k_opt = 6 \n",
    "kmeans_final = KMeans(n_clusters=k_opt, random_state=42, n_init=10)\n",
    "df['cluster_kmeans'] = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nK-Means finalizado con K={k_opt}.\")\n",
    "print(f\"Silhouette Score (K-Means): {silhouette_score(X_scaled, df['cluster_kmeans']):.3f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 2.1: Implementación - DBSCAN\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Definición de rangos para la evaluación empírica\n",
    "eps_range = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "min_samples_range = [3, 5, 7, 9]\n",
    "\n",
    "dbscan_results = []\n",
    "\n",
    "for eps in eps_range:\n",
    "    for ms in min_samples_range:\n",
    "        db = DBSCAN(eps=eps, min_samples=ms).fit(X_scaled)\n",
    "        labels = db.labels_\n",
    "        \n",
    "        # Cálculo de métricas\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "        \n",
    "        # Cálculo de silueta (solo si existen al menos 2 clusters)\n",
    "        if n_clusters > 1:\n",
    "            score = silhouette_score(X_scaled, labels)\n",
    "        else:\n",
    "            score = -1 \n",
    "            \n",
    "        dbscan_results.append({\n",
    "            'eps': eps, \n",
    "            'min_samples': ms, \n",
    "            'n_clusters': n_clusters, \n",
    "            'n_noise': n_noise, \n",
    "            'silhouette': score\n",
    "        })\n",
    "\n",
    "# Convertir a DataFrame para comparar resultados empíricos\n",
    "df_eval_dbscan = pd.DataFrame(dbscan_results)\n",
    "print(\"Evaluación empírica de parámetros para DBSCAN:\")\n",
    "print(df_eval_dbscan.sort_values(by='silhouette', ascending=False))\n",
    "\n",
    "# SELECCIÓN FINAL: Aplicamos los valores identificados como mejores\n",
    "# Basándonos en los resultados impresos arriba (ej. eps=1.5, min_samples=5)\n",
    "best_eps = 1.5\n",
    "best_ms = 5\n",
    "\n",
    "dbscan_final = DBSCAN(eps=best_eps, min_samples=best_ms)\n",
    "df['cluster_dbscan'] = dbscan_final.fit_predict(X_scaled)\n",
    "\n",
    "n_clusters_dbscan = len(set(df['cluster_dbscan'])) - (1 if -1 in df['cluster_dbscan'] else 0)\n",
    "n_noise = list(df['cluster_dbscan']).count(-1)\n",
    "\n",
    "print(f\"\\nDBSCAN finalizado con eps={best_eps} y min_samples={best_ms}.\")\n",
    "print(f\"Número de clusters encontrados: {n_clusters_dbscan}\")\n",
    "print(f\"Número de puntos de ruido (outliers): {n_noise}\")\n",
    "print(f\"Silhouette Score (DBSCAN): {silhouette_score(X_scaled, df['cluster_dbscan']):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PASO 3: Visualización y Evaluación de Resultados\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Reducción de dimensionalidad con PCA para visualizar los clusters en 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "df['pca1'] = X_pca[:, 0]\n",
    "df['pca2'] = X_pca[:, 1]\n",
    "\n",
    "# Gráfico Comparativo: K-Means vs Tipos Originales\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.scatterplot(data=df, x='pca1', y='pca2', hue='cluster_kmeans', palette='viridis', ax=ax1)\n",
    "ax1.set_title('Agrupamiento por K-Means (K=6)')\n",
    "\n",
    "sns.scatterplot(data=df, x='pca1', y='pca2', hue='type1', ax=ax2, legend=False)\n",
    "ax2.set_title('Distribución por Tipo Elemental Real')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparación estadística: ¿Se alinean los clusters con los tipos?\n",
    "# Usamos el Índice Rand Ajustado (ARI). 0 = Aleatorio, 1 = Coincidencia perfecta.\n",
    "ari_score = adjusted_rand_score(df['type1'], df['cluster_kmeans'])\n",
    "print(f\"\\nÍndice Rand Ajustado (Coincidencia Cluster vs Tipo): {ari_score:.3f}\")\n",
    "\n",
    "# Guardar resultados para la memoria\n",
    "df.to_csv('resultados_clustering_pokemon.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
